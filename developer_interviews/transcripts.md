# Anonymized Interview Transcripts

## P2

When I commit the lockfile into the repository, I typically expect all the dependencies to be exactly the way that I have it set up. And I expect very minimal breakage. I'm going like, if I, you know, if I check out a commit from six months ago, and I try to try to build it and run it, I expect it to work. And with a lockfile, like it almost always does. So it allows me to essentially step back in time and see like, all of the dependencies and all of the sub dependencies and everything. It allows me to make sure that it's essentially like a snapshot. Like if I set up like a Docker image for every single commit, it allows me to like emulate that. Like I'm 100% certain that it's going to be identical no matter when I go back to it. So, yeah. I don't normally look at it. I just add it to the commit and I push it. 


I'm kind of the only like real maintainer. Other people for sure contribute and I, you know, pull those in. But I'm working on it alone, like day to day. So I'm typically not like trampling over anyone else's changes when I commit my lockfile changes. But if I was working with others in a professional capacity where I was working with other people, I would want it to be sort of expected behavior that everyone commits it at the same time as you commit the other change to like the toml file when you're changing the, like if you change the version or you add a new dependency, I expect that to be in the same commit as the lockfile change. And so then it's much easier to deal with merge conflicts. As of right now, there's no real like procedure because it's sort of just me working on it. So there's no like conflicts. But if there were others, it would be it would be pretty easy to make sure that that didn't cause any problems as long as everyone made sure to commit it. And like we all follow that behavior. 

Default, I'd say would be to just run the cargo add command, which will put the latest version of it. There will be some cases in which I have to pin it to a specific older version because the newer version has breaking changes that I haven't had the chance to sort of patch up and things like that. But most of the time I don't, I rarely edit the toml file directly. I'm running things like cargo add and then like adding features with that command. Just because when I'm at, if I were to like type in a dependency, I wouldn't typically know what version I want. I just want like the last version. So I use the command to edit the toml file. 

I typically run cargo tree and then I just search in the output there. I'm not, I've, now that you mentioned it, it's probably better to look at the lockfile just so that you can see like all of them at once instead of having to scroll through the output. But when I do it, I usually just do cargo tree to see which, it allows me to see which dependency is a dependency of which dependency. Like sometimes multiple versions will exist, like one pack, one dependency uses one version and another dependency uses another. And I'd like to say like, okay, I want to get rid of the one that uses the older one or update it. And the lockfile, as far as I know, doesn't tell me like who wants this package. Or if it does, it's not as easy to parse as just like the tree command. So I typically use cargo tree. 


Typically what I will do is I will recreate the, I'll pull their change into a branch. I will recreate the change that they did to the cargo.toml and then see if my lockfile changed in the exact same way. Just because often I don't know exactly if this should be a dependency of that. But if when I add it, it adds another dependency. You know, if I add X and it adds Y and they added X and it added Y, you know, then it matches and that's okay. I don't typically vet every dependency that I get PR for. But when I do get a new dependency, I want to make sure that like nothing is sneaking into the lockfile. I'm not, I'm not even sure if there's an attack vector there, if like downloading a dependency will necessarily cause any problems. But I will make sure that the change to the lockfile is identical to the change that was caused by the change in the toml file. 

So typically what I do in the CI CD pipeline is it builds and does a release on the CI CD from the latest commit. So it will use the latest lockfile. None of my CI CD's sort of install it from cargo. Like they don't run cargo install. They typically build and run the cached version of it. And also like users, almost every, I'm pretty sure almost every user uses the Docker image that I put out. And I personally think that that's a much better, much better distribution mechanism just because most people are using it in a server capacity. So Docker is just like naturally better for that. I'm sure there are some people who do cargo install. And for the most part, even if you ran cargo update, it wouldn't cause any breaking changes. I make sure that running cargo update, I run cargo update every once in a while. But even if you did that without locked, I'm fairly certain that it will still work just fine. Because if there's something that I need to hold back, I define it to be held back in the toml file. And so it still holds without the lock file being checked in. 

Definitely. I do a lot of cargo tree, which parses a lock file. I do that to make sure that dependencies, if there's like a really heavy dependency, I can make sure to find a lighter alternative. But also just to see what versions it's pulling in. Cargo update, of course. Cargo add and remove. Yeah. That's about it. The rest of it I do manually with like things like features and stuff. That I edit by hand. But mostly those few, yeah. 

I've had no problems with the cargo lockfile. It's all worked exactly how I expect it to. 

I mean, outside of the fact that certain dependencies will break things and older ones will, you know, either work or not work depending. I don't think there's been any problems that were like caused by the dependency system in cargo. Yeah, it's all pretty, it's well built to like match my needs. If I need to set my dependency maximum to a certain amount, like I don't want to go higher than this. I use the equals, you know, things like that. But there's been problems that I've had pretty frequently actually with hyper. It's on like version one at least, but we're stuck on 0.14. And so I need to make sure that like all of the dependencies don't go past that. But that's pretty easy to specify. I just say hyper equals 0.14. So I haven't had any problems with dependency management itself. There's been problems with dependencies. But like that isn't the dependency management within cargo. I've had no problems with that. It's all working basically perfectly. I haven't had to think about it much. 


I think that this isn't relevant to me necessarily because I commit the lock file. But I know that in some cases if you don't commit the lock file and a dependency gets yanked, you cannot build it anymore. Because it may exist to people who have the lock file. If you have the lock file, you can always build it, even if it's yanked. But if you don't have a lock file and it gets yanked, you cannot under any circumstances build it. And it's to me, I think that that's the that's a huge oversight that like it's purely just cargo telling you you can't do it. There have been people who like patched out the three lines that check that stuff. And they're like, okay, I can build it now. Like what's now that I have a working lock file, I can, you know, go back to using regular cargo. But like to me, if I didn't commit the lock file for some reason, and I had an old version of a dependency yanked, that would be really frustrating to me to be unable to build it. And that seems kind of like not along with the rest of Rust sort of dependency management. And that's not just like a like yanking dependencies is not uncommon. There is the really core dependency of Ring, which is like a very, very common cryptography library that was maintained for a while. And they had a they had a policy that they would yank the every time they released a new one, they would yank the previous one. So there would always be constant broken versions of things and people have to patch out the lock file and things like that. And it was they changed the policy of that package. But like the root problem still exists that if dependencies yanked and you don't have a lock file, you know, you can't do anything about it. You can't build it. You have to figure out a way to like wrangle it to let you do what you want to do. Maybe that's not relevant because that implies that you don't have a lock file. But I think that, yeah, I think that that would be one problem that like I have run into outside of this project. 

I do not personally use Nix. It was a PR that someone added so that it could be added to I guess Nix is equivalent packaging system. But I have heard of it, Nix, and it looks really cool. I just haven't done the work to like actually adopt it myself. 

## P8

This interview was conducted over coffee for about an hour. The conversation was not recorded. Only written notes were taken.

Challenges when managing dependencies
The dependencies get unmaintained - no bug/vulnerability fixes
When the organization responsible for maintaining the dependency get acquired by another company 
Complex coordination problems 
Several versions of the same dependency  
Security of the dependencies
Sanity check - when bumping dependencies existing tools are not good enough recognizing the malicious updates
When a dependency has released only the binary to improve the compile time - nobody cares about verifying the maliciousness 

The reasons for not using lockfiles
As bevy is a game engine, on consumer side using a lockfile can cause difficulties
Better user experience is more important than the developer experience 
Cargo cache mechanisms are buggy, therefore, not caching is the standard
Automated frequent dependency updates are more important than reproducibility

Other tools used to support dependency management
Cargo semantic version violations checkers
Cargo tree to visualize the transitive dependencies

Suggestions to improve dependency management processes
Use only trusted package repositories when shipping binaries
Verify that the checksum of the code on github matches the code on crate.io 
Socio technical solutions
Shared partial maintainship of dependencies 
Verify the reliability of the package by verifying the maintainers of it
Have a mechanism of secret sharing or shared keys between the maintainers of a dependency
Bypass crates.io have a trusted managers

## P6

I don't manually check it unless I'm reconciling it, unless someone is contributing a package change. So, then I'll look at it and see, because sometimes there will be artifacts from their operating system, and I need to make sure that it was installed correctly, too. Because, you know, maybe it's different project to project, but in my specific project, you need to make sure to install it in specific folders and things like this. 

Yeah, I'm going to take a look at one right now. So, mainly the version, and I'll look at what the resolution link is. Sometimes there's, I forget what exactly it's called, but whether something is marked as dev, if it was correctly installed as a dev dependency. 

Yeah, I use some of the NPM scripts. Like, you can run NPM ls and then write the package specifically. Specifically, I guess it sort of is part of my debugging process, but not so much of me looking at the file, but seeing if everything is still running correctly, like the installation process. Like a verification, but not exactly as a debug.


Yeah, there was a time where I, so, there's one specific package called the rollup, and that's a very common web compiler type of package. And it's, once in a while there, there'll be some very specific, hard to debug problems with that package. And I needed to manually, I was trying to install a specific version, but I was running all the installation scripts, and it still wasn't installing the one I wanted. And so, I kind of manually had to change each link. It was, I don't know if it's a, I don't know if that's a common thing people do, but in that case, I needed to do it. 

Yeah, I don't, I don't think I've ever deeply looked at it. Um, I'm, I mean, I'm sure it's used in the automatic process of installing and all this, but, uh, I, I personally, if I have benefited from it, it's been indirectly. I haven't paid careful attention to it myself. 

Yeah, so I'll manually update the version on there of the app. Um, but really it, it only gets changed, um, uh, with, with package updates. So , um, once, once in blue moon, I'll, I'll have to rebuild the package lock, uh, lock file, like in case I'm having trouble installing something, like, and just compare differences that way. But I, I would say those are the only times. 

Well, it's, so when, well, either when we install a new package or update the version of an existing package, Or even, um, or even as a result of a security advisory, you know, npm has this automatic process for that. So... Yeah, or do it. Okay. Um, so those, those are really the only times, and just manually updating the version of my app, um, at the very top. 

Yeah, yeah. I do enforce, um, uh, adhering to the package lock file. Um, we use npm-ci for all the CICD. That's very important because, um, I even have specific tests that run in a Windows environment, for example, as opposed to, like, normally it's just running on Linux. Um, because certain, certain dependencies wouldn't resolve correctly if, if we didn't enforce it. 

Yeah. Yeah. I, you know, I ran into a situation where it wasn't work, my app wasn't working for Windows users and that's why there's that workflow now. Uh-huh. Um, and early on in the project too, we were trying to set up end-to-end testing automatically with Playwright. And, and someone had contributed that workflow. Um, and it worked for them locally. And I saw that they were using npm install instead of npm-ci. So, um, that's one specific situation where I saw, oh, it's, doesn't seem to be working in, in GitHub servers for some reason. And, and, and that was part of the debugging process. You know, it really, it helped us realize, you know, what was going on. 

Yeah, so I, I, I use the npm cache clean. Um, basically anytime I'm testing the app in general, I actually wipe, I have a script that wipes the node modules folder and runs a clean installation that way, cleans the cache, and then does npm-ci. And that's, that's the, the true and tested way of getting a, a, a clean copy of the app every time. So, uh, it's just built into even my development workflow. 


Yeah. I mean, um, just working across different operating systems, um, you know, also on macOS. I've had issues where deleting, like doing that same process, um, deleting the node modules folder created issues with npm and then nothing would install. Um, and just, I've, I've had a lot more issues, I think, with also just the installation process sometimes, like hanging, um, for whatever reason, or not resolving dependencies. Um, so there, so there have been, I think the most common issues I've had are OS related, like I was on Windows or I was on macOS versus Linux, or some kind of connection issue with the, with npm that messed it up along, along the way. 

Yeah, that's, that's a hard one. I would say, uh, it's, it's hard because sometimes, um, I, like some of the things that I think they have built into it, that wasn't always there have been helpful, like, like being able to override package, the package lockfile. Um, like on the package JSON file, you can set it in an overrides field and it can say exactly, uh, which version the lockfile should use. Um, that's, that's been helpful in certain situations. Um, maybe an understanding of, of how it constructs the lockfile. Cause it's still kind of a mystery to me. Um, and, and maybe some way, uh, um, and NPM does, does a good job of this already, but maybe some way to not just security vulnerabilities, but maybe issuing warnings with specific problems that are happening with packages. Like maybe, maybe the package maintainer can, can issue a warning, like certain users are having trouble on this specific environment or whatever it is. Maybe some kind of feature like that. Um, I don't know if that's related to the lock file construction itself, but I, I know it's, I mean, some, it's, some of these issues I've had with it are just so hard to debug. And, uh, until I do some very thorough digging of what's going on. Um, so anything to help with that, I think would be appreciated. 

Yeah. I mean, I've, I guess I've, I've had situations where I could look back in my Git history and see, okay, well, this package was working, um, when the lock file looked like this. And for some reason, when I rebuilt the package lock file, um, later on, it wasn't working anymore. So, uh, that was one time I literally manually copy, copy pasted. I went exactly where, um, that package, how that was resolved before and it was working and I literally replaced it. And so, okay, like it needs to be this version. Um, yeah. So in that case, you actually, um, enforce the, uh, look for in your GitHub backhands, right? So that's, that's why you had to copy it and then it was resolved. 

Um, yeah, the ideas and. Yeah. Um, I think, uh, I think it's interesting to see new, uh, new, like JavaScript environments that kind of have their own, uh, package resolution strategies. Like, like the bun runtime is pretty interesting too. That also has like some kind of lock file. Um, so I think, I think NPM is still great. And I think maybe it could learn from these other, uh, uh, these other runtimes or, or services like, uh, you know, and JSR is another one that's new. 

Yeah, I, I, I kind of, um, I still mostly build with NPM, but I use, I, I also layer Bun on top of that. So I, I like to use NPM as, as the, the source, so to speak, like the, the base. And I try to build things that work for sure in an NPM environment and then go with Bun from there to like benefit from increased runtime speeds and things like that.  


Yeah. It, it depends how much changes the contributors is, is, uh, is pushing. And, and I am really careful with that to make sure that, um, that there aren't any merge conflicts in the package lock file because it can be, it can get very messy. And there, there, there have been times where I, um, um, it's mainly, uh, maintaining the repository, like the main repository itself. It's not usually a problem. But I have, um, I have helped people maintain forks of my project where they install, you know, like their own dependencies and things like this. That aren't in the main project. And so there, it's more of an issue because you have completely divergent packages. And usually if something is too much to reconcile, like, like I, I, I have a pretty decent computer and, and, and even like in trying to merge changes, like it's the Git, Git is kind of overloading and taking up a bunch of memory.  . And is even having trouble showing them all.  So in, in situations like that, I just, I just rebuild the package lock file and, and I test everything in the app and see if it's, if it's still functioning as expected. That's, it's, it's a bit of a destructive process, but, Yeah . But it, it, it usually helps me more than, more than hurts. 

Yeah, absolutely. Absolutely. I, I, I do care. Um, and so for the main repository, I almost never do that where I rebuild the whole package lock file. Um, and it, it's very important to keep it reproducible. Um, and for that reason, I also use, I, I also heavily rely on Docker to be like one of the number one ways people, um, are able to even use the project. Um, cause that way there's really one source of truth. Um, and I'll even host the image that Docker produces. Um, yeah. Um, and, um, uh, so even with, uh, so you, uh, rather like think that the Docker image would be your application inside the document would be more reproducible. 

Yeah. Um, well, mainly because I work on the, the project is so centered on GitHub. I use their, mainly a lot of their tools and, um, mainly that like they, they have a security section for the repo. And that also alerts me anytime I make pull requests. Um, that kind of scans the package lock file, um, automatically. 
## P7

Yeah, it's committed. And I can give you a brief reason why I commit it. Because I'm using Rust for the project. And as Rust suggested, the official guidebook suggests to commit files for executable and to not commit it for libraries. This is because usually for executables, you want to log to a specific version of each package to not break everything. Because basically, executables are only used by end users. Instead, I also maintain other Rust libraries. And in libraries, I don't commit it because, you know, libraries are used also in other projects, also in other libraries. So it's not a leaf dependency, but it's something used also by other developers. And usually to not break their dependencies, it's suggested to not commit it. But yeah, with this, I am because basically, it's a binary. 

Yeah, usually, to keep dependencies up to date, I use Dependabot on GitHub, that you probably already know. So basically, unless that, of course, when I add a new dependency, because I'm adding new features, I do it manually. But for updates, they are just submitted daily by Dependabot. And I just merge the pull request that is also updating the lockfile. there is also cargo update, of course, which I also use that. Yeah, so it's either via Dependabot or cargo update. 

No, I only check it if there is some problem, some conflict, you know, the dependency. But in normal scenarios, I don't check it because of course, it's supposed to be generated automatically. So if everything works, I think there is no real reason to check for it. So if you just ran into some kind of a problem with dependencies, like breaking updates or something. 

Yeah, I use both. But as I mentioned, this is the only case where I check it visually. Actually, it happened once that I had the dependency conflict. And this was due to two different dependencies using another dependency. And the way I solved it was just by restricting the features of one of the two dependencies. In Rust, every library has some features that can be enabled or disabled. But I didn't need all the features of one of the dependencies. So I had the set of features and I was able to solve it. But yeah, usually I check the lockfile. 

Yeah, because I don't trust anyone. Yeah. In practice, what I do is, of course, dependable. I just review toml the file, but usually it's just a version bump, so it's just a digit. But if there is some users and it happened that they had new dependencies in, not in the lockfile, but in the manifest, of course, the lockfile is updated as a consequence. But since I prefer to double check, I clone their fork locally, and I run card back date, so that I'm sure that the lockfile is reflected on the actual changes, and, you know, maybe they had something malicious, so it's better to check. 

Yeah, I just check if it's the same with git diff, yeah. 

Yeah. When I use in development, I don't use it, but it happened one time, a few months ago, that there was a library, one of my dependencies, that was updated as a minor version, and as you know, of course, minor versions usually don't break stuff, but in that case, it breaks stuff. So it wasn't a minor version, even if it was dogged as that. So basically, it was a same problem. Semantic versioning was not respected by them. And I got an issue by a guy that had some problem. So I updated the readme, and saying that if they download from cargo, they should use cargo install dash dash locked. Because in that way, I'm sure that they're using the dependency in the lockfile. Because otherwise, cargo as default tries to pull new versions, even with binaries. So even if you have pushed the lockfile, cargo still tries to, to ignore it. So you have to, to specify a lock in order to use it. 

No, I'm not forcing it because usually, I'm keeping the lockfile always up to date. So locking it doesn't make sense because it's always the last day latest version. 

Yeah, it was a problem that I mentioned before. Basically, there is this library that is a profanity check that I'm using because I'm parsing large external files. And I'm checking if the word contained in these files are not something weird, you know. So basically, they updated this library, pushing a minor version change that was updating the list of words to check against for profanity. And so this was a minor version, of course, but they updated the words and I got a fake positive. So it breaks something up. Yeah. But yeah, I think that in these cases, the situation can be solved by using the locked flag. 

I'm not sure. Maybe cargo clean when there is some problem. But yeah, I think that's it. 

Usually, it's because I did some error, but because the compiler is never wrong. So probably when something is not as expected, I retrace my tabs because I think that I did something wrong. So it's my fault, not the compiler. 

I've never actually checked checksums. I mean, the only times that I suspect is when someone else is updating my code. But again, I think it's enough to run cargo clean and cargo update. And so yeah, basically, I'm trusting cargo. And if there is an upstream problem in cargo, I get affected, of course, because I'm trusting, I'm relying on it. 

Yeah, I think that. Yeah, probably it'd be nice if the lock file was automatic in case of binaries. So if the cargo lock is published, I think that a good default might be to use it actually. Because the first time I didn't expect this behavior. And then reading the documentation, I found out that the cargo lock is ignored, even if it's present, even if it's present, if you're not using that flag. But other than that, I have no particular concern. 

## P1

Generally not. We do not. We do not. There are very few cases where we need to. The reason why we have to do that is mostly because we need to investigate some deep dependencies that we might not be aware of. Again, it's mostly for security reasons, but we can use commands such as NPM list, which gives you the list of dependencies and that can explore relatively deep inside the dependency tree to see where the components are used, which ones are deduped and so on. So, but it's pretty rare. I mean, I don't know if you looked into it, but it's quite wild what you can find inside the package lock. So it's not really readable. 

No, we do not. We do not check the lock file. I would say most of the time, the reason why the lockfile would change is because of a security update. And we have some trusted tools, which does the right updates. So we do not have to investigate why the package lock has changed. So it's mostly because of security updates. 

There is no strict guidelines about when or how we should push the package lock. But as you mentioned, every time we have a new dependency to a project, first you modify the package.json, then you do an npm install, and then you have the package lock being updated. So you push that. What we try to do at SAP, at least, is, well, in my team, we try to minimize the content of a commit. So if we have to add a dependency, we will add the dependency, commit that, and then work on what we need to do. So we try to have small commits. Beside that, I mentioned that we have some security tools which are going over the list of dependencies. So those tools require access to the package lock. We recently had some troubles with the package lock. Because again, for security reasons, we try to keep up to date with the latest version of node. So as you may know, the package lock has, at least we are in version three now. So there are like three different, well, not really syntaxes, but three different versions of the package lock file. I know also that you can change the dependencies are either deduped or not through some configuration in the npmrc. But we don't do that. We just apply the defaults. So to answer your question, there is no strict guideline on when to push the package lock. But we try to do that in small commits, at least. We don't have a huge commit with lots of files plus the package lock. Otherwise, it's a bit confusing what has been done in this case. It's a bit confusing what has been done in this commit. 

There are many ways to answer your question. So first of all, we clearly distinguish the production dependencies from the development dependencies, which means you know in the package.json, you have two sections. Well, actually, you have three if you consider the peer dependencies, but we don't use peer dependencies. So the dependencies are the dependencies used only for development. So those dependencies should not be pushed to production. Typically. So what we do is we have a clear distinction between production dependencies and development dependencies. When we go through the pipeline in order to test or to run our tests, typically we use npm ci to install the dependencies by trusting the package lock. And also, since we are developing microservices, we have Docker files where we build the images and the image to be pushed onto the image to be pushed onto the GCP platform. So this Docker file also ensures that we grab the dependencies from the package lock by using several stages. And the first one is typically to use npmci to get the dependencies. I'm not sure I was clear. Sorry about that. 

Oh, well, I would say the most basic requirement is whatever is needed to run the production dependencies, obviously. But it's true that there are some knowledge that you have to be aware of. For instance, we use TypeScript. So TypeScript is a transpiler. So typically when we go to production, we don't need TypeScript anymore. Instead we just push the build result. So the dist folder or whatever you want to name it, which contains only JavaScript files. So typically TypeScript is a development dependencies. The same way, everything that is around linting, testing, what else? Some build tools also sometimes for Vue, for instance, we have VTest, we have VIT, which is a bundler to produce the application. So those kinds of stuff are development dependencies. But sometimes there is a thin line between a package that might be used both in production, but not everything is used for production. But since it's bundled in one package, we don't have the choice to just put it. So for that, we, from time to time, at least for the UI, we use tree shaking so that it reduces the dependencies. And we have something that we have something that is bundled that contains the whole JavaScript. So we don't have any more dependencies. So that's not a problem. But for services, typically we try to distinguish what is there from prod according to the fact that we put only what is required on prod. I hope I answered the question. 

I will be honest with you. I never looked into that. I know that some teams at SAP don't trust the official NPM registry. That's the way we name it. So we have a sort of clone owned by SAP, which is called Artifact Repository, something like that, where you can configure NPM to grab the packages from this registry. So it's like a trusted source of packages. But at least in my team, as of today, we rely fully on the public NPM. So we trust that the fact that the checksums are being validated, but we'd never really investigate that. We just trust the system. 

Yes, sure, sure. So we have a lot of security tools, obviously. So the first one is called Black Duck. So Black Duck, what it does, it takes what we call the bill of material for the package, so for the product, which is more or less the package lock. And Black Duck uses a list of vulnerabilities based on the package names, on their versions, on the reports on a regular basis on our products to know if one of the dependencies of this product is having a known vulnerability. So that's one thing which leverages the package lock.json. We also use Dependabot, which does that proactively. So Dependabot will also have its own list of vulnerabilities. And if it detects that one of your packages is vulnerable, it will automatically commit an update of the package lock, trigger the CICD. And if everything is good, we just merge automatically. And we just recently moved to Renovate. I don't know if you know it, but Renovate is also quite an impressive tool, very configurable. And that does almost the same thing. Just go over the list of packages and is capable of not only for vulnerabilities, but also to keep up to date with the latest version. Because if you're familiar with the topic, there are two main problems with dependencies. First, obviously, the security vulnerabilities, but also from a maintenance point of view. If you rely too much on an old package, then you have the risk that at some point this package is deprecated for whatever reason. And then you're stuck because you rely too much on this functionality. So we try to always stay up to date using those tools to make sure that we have the latest version being run in production.

well, I have a good example in mind actually, because one of our practices is to try to avoid the copy paste across the code everywhere. We built a library. So this library comes with its own package log.json. But we recently realized that when you embed the library in another project, the package log of this library doesn't necessarily influence the dependencies you will get in the project where you use this library. So I've read about, there is, I forgot the name about the file, but there is another file that is really similar to the package log, but that has a different name that you can publish with your library so that you can impose the dependencies. I forgot the name, but it's not named package log. So you can produce this file so that with the library comes a sort of fixed list of dependencies that you have to respect. So that's one thing that I observed recently, and we're looking into that. But because of renovate and everything, we're still up to date. So that's not a big problem. And another example I have in mind, I had some troubles with dedupes, for instance. It's sometimes happened that we have to use overwrites in the package lock.json. And I have to be honest, sometimes it's really difficult to understand how it works because it either works or not work. And you don't really know why, because there is no clear debugging tool to understand how this list of dependencies is being built and resolved by an NPM. So that's the two examples I have in mind. 

Well, I would say that one thing that could be interesting is, I'm pretty sure you've faced that, is whenever you have problems installing NPM packages. So it may happen from time to time that you have things like a package which declares a specific version of the engine or a specific version of Node.js or a specific version of NPM itself. And sometimes it breaks the installation. The error message that you get is really cryptic. Like, you really have to dig down the traces a lot to understand what's going wrong there. And I believe it's related to the complexity of what NPM does. But the problem is that when you're not too much into, I would say, if you don't necessarily understand precisely what's going on, you're completely lost. And I would say most of the time what people do is they delete the NodeModules folder, they delete the package lock, and then they retry and install. And then they see if it works. And if it doesn't work, then they change something in the package, they try again. So I would say troubleshooting NPM is probably the most complex things I've ever seen in Node.js environment. Well, there are other things which are complex, but related to what we're discussing. So hopefully, whenever it goes right, it's okay. It's a nice tool. But as soon as you've got a problem, it's a mess to figure out what's wrong. So deduping, overrides, things like that. It's really complex to understand how it works and how to make it work when you've got a problem. And that's really my pain. And hopefully, or luckily, it doesn't happen too often, I would say. 

## P3

So usually the issue with dependencies from what I experience is usually when I'm working in my local, it's working just well. But when I run in continuous integration, like GitHub actions or some... I don't know. There is error on the server and the continuous integration. So I think it's related to the dependencies. And usually I find what kind of dependencies is that. It's not... What's the difference with my local setup and the server.
 
Usually I just see the... what kind of error. And then from the error message, I just kind of search what this error means and which of the dependencies that trigger this error. So usually after... After finding some information, then updating the dependencies, one of the dependencies, can solve the issue. 

I'm not checking the lockfile manually. 

I prefer to just use the npm and then use specific tools to manage dependencies. 

Yes. It can be a... if the problem is in the... in one of the dependencies... I maybe... pin... pin... In npn there is a package JSON that we can pin a resolution... to the specific version of dependencies. So if the code is only running with the specific version, can be using the resolution. Yes, because in the lockfile, there are so many dependencies of the dependencies. And the dependencies, it's kind of too many. But in the package JSON, we can know which of our dependencies could be the problem. 

I don't usually check the integrity because in my opinion, it must be checked by the tools that I use. So I don't usually check the integrity. 

Yes. That's my concern as well. The reproducibility is because especially when committing a new version or committing specific tags, we have to make sure that our local setup will be the same as others. And the details of the dependencies, the version will be the same. So it will run the same as what we built here and what others built. So it's a must have, I think, to prevent what kind of problem that might be not because of our code, but it might be some of the dependencies from our code to prevent that kind of problem, I think.

Yeah. So, even in the local I usually use just npm install and at the npm build lockfile automatically. So, for the continuous integration, I'm using the npm ci. So, it's just pulling what it doesn't build again. So, it just run like usual. 

Maybe the versioning of the dependencies with current setup? I cannot know which version is the latest and how to automatically in update specific package that we want to put without mentioning it in package. But maybe some tools that can automatically update to the specific version. 

## P9

Not unknowingly. In Rust, many crates are under version 1.0, so semver allows breaking changes when bumping just the minor version. My personal feeling is that the Rust community takes semver seriously, and I'm looking forward to improved tooling like cargo-semver-checks.


This was the previous recommendation Should Cargo.lock be committed when the crate is both a rust library and an executable on StackOverflow, but has since changed. I have not revisitted this choice, but if I were to start a new project today, I would follow the latest recommendations and defaults.

Not in my past open source projects. My preference is to avoid dependencies as much as possible, so generally I don't run into issues by not using exact versions. In my work environment, we do use lockfiles, and they are valuable, as it ensures the dependencies' code doesn't change unwillingly. But it is also important to be mindful when updating the lockfiles, and changes to them should be reviewed as if it was any other part of the code.

cargo upgrade sporadically, but this also changes the Cargo.toml. Whenever I do `cargo update`, it is most likely an accident, and I meant to use the other command. I generally don't care about having exact versions when developing my open source projects, as long as the versions that get installed are compatible.

For open source work when I'm not committed to delivering strong promises, I pretend lockfiles don't exist or don't give them much thought. It is something that works transparently, so I don't dedicate much attention to them.

Cache invalidation is a big issue when using certain package managers in other languages when there is a need to work from multiple dependency repositories for security reasons, often requiring disregarding the previous lockfile and generating a clean one. But the impact of doing so is minimized due to lockfiles being reviewed.
When the existing lockfiles is deleted and a new one is recreated, this is effectively "trashing" the old information. But since SCM tools like git treat it as a change to the same file, the changes can be reviewed, and I believe it's important that the file is reviewed (and not just ignore the changes that happen on it).

Grammers has tests to enforce that all dependencies used by the project have some documentation on why they are needed. This makes the process of adding additional dependencies require more thought and rationale, so the number can remain low. I find deterministic builds interesting, but have no personal use for them in my current projects. Since I'm the main owner of the code, I know what the built binary will do (provided I also trust the dependencies to play nice).

For binaries and dependencies intalled with cargo, I rely on the automatic checks it performs (if any). It's mostly trust, though.

The dependency tree and final binary size can quickly explode if dependencies are not kept in check, which is why I favor re-implementing parts as needed.

I think cargo tree is great at providing a good overview, a well as the aforementioned semver checks. But it would be nice if cargo-crev was more popular.

Only as a way to count how many direct and transitive dependencies it has installed in total.
## P4

Yeah. So we have very few dependencies because it's always very painful to have many. And the ones we do have are still painful. So like, that is something that I'm actively working towards, like getting rid of the last dependencies, because that's just, it's always annoying to have them. And like, for example, one thing is that we have backwards compatibility until node 12 and node 12 didn't have like a global fetch function. So one main dependency is node fetch, which basically just adds something that all later, like all recent versions of node already have, but it would be a breaking change to drop this because it's a very core feature. So it would kind of like change the API. So we can't drop it. But also the maintainers of that thing have decided to drop support for common JS. So now we can't upgrade their dependency anymore. And we also can't get rid of it. And like, it's just constantly an annoyance. And there's not really much we can do, actually, we will have to do a major version and do breaking changes and throw it out. And so that is kind of the deal for all dependencies. Like there's no, or at least I haven't figured out the general rule, how to fix these things, you just have to understand why do you need it? Do you really need it? And, and then look into the specific issue. We didn't have any security issues so far yet. 

I follow all the changes very closely. It's, I mean, it boils down to about it, but it's not having a good review process. Um, I don't think it has ever happened in the past three and a half years that there was like an unused dependency ever. So we are all very cautious about dependencies and, um, because they're so annoying, everyone's trying very hard to get rid of them. Yeah. Uh, but I do know of other open source projects that don't work that way. They pull in like a million dependencies and they don't really care. Uh, that is not how we work. 

Um, because, uh, so there's a difference between library software, like what I write and then application software, which is actually a executed on servers, like directly as the main application, because when you install the dependencies of your project, then the lock file will basically track. Every single version of the entire dependency tree. Right. But it will disregard any lockfiles of these dependencies. So basically if we were to commit a lockfile, it would be ignored by everyone who uses our library. So there's no point really in adding it. And, uh, uh, even worse, if you do add one, you kind of lock all your dependencies to one specific version. So everyone who works on the project, they all have the same version, but all the libraries, library consumers might deviate from these dependencies. So basically what you're doing is you're not exposing yourself to all the different combinations and all the ways how your dependencies will actually be installed. So basically it is better not to commit a lockfile because then you kind of run into these issues faster. And, uh, yeah, you don't have this false sense of security where you think it's all locked, but in reality, everybody who uses it doesn't lock it. 

Uh, I haven't done very thorough reviews of the dependencies that were added and their dependencies. So I kind of just trusted the people who did the change to do these checks, which probably is not ideal, but, uh, yeah, it, it did not really happen that often. Um, so the, the main dependencies that we, when we integrate with database drivers, then we often need these official database drivers as peer dependencies. And so we kind of just trust them anyways. It's kind of like this, this idea, okay, you install whatever postgres and then integrate with postgres. Then we kind of need postgres too, in order to integrate. But at that point, the application developer already has it installed. So it's not like we are adding anything to the tree. Yeah. And the other, the only other case I remember where somebody added the dependency basically for convenience. Uh, it was actually removed again before the release because we figured it's not worth the, the overhead. So yeah, we don't really do it as much. 

No. Uh, well, okay. So this is a bit special. There are a few places where we do have a package lock file, but it's not a lot of work. lockFile, but it's get ignored. So everybody just has a local version and we don't really know what people use. Um, it might be like, for example, in continuous integration, it's regenerated all the time. Uh, but then again, we don't really use NPM as the primary way of installing dependencies. We have like this, um, hybrid support for both Dino and node. And so like with Dino, you just, uh, we basically disabled the lockfile. So it's just not generated. And then, uh, when we do an NPM publish, it kind of gets created automatically in CI, but you're like, uh, on the core repository, I don't have a lockfile locally. I don't have NPM in there really.

pretty much just trusting it. Uh, yeah. Uh, yeah. It's, it's not ideal, but it's, yeah. No, there's a, there's one thing that's very cool. It's called JSR and I don't know if you've heard about it, uh, but it's kind of like this new thing that tries to replace NPM. Mm-hmm. And what they have is this, uh, security feature where you can, you can build your dependency, like your, like compile it in GitHub actions. Mm-hmm. And it will actually, uh, show, like a verified badge when you download the dependency on JSR. Uh, so it knows that the, the build output hasn't been tampered with. So as long as you depend on JSR and do things only from there, then you can basically automatically get this software bill of materials and know, like have like a verification for every single dependency. That is very cool. We're trying to migrate there, but it's not really that mature yet. Uh, it has very cool features, but it's, it's like, there are a few things that are missing. Um, so the whole security aspect will get a lot better once we move away from NPM. 

I, I wouldn't know exactly. Yeah. Um, so what Dino does is it uses URL imports. So like you just specify the URL of whatever you have. Whatever you want to import in the, in the import statement. And then it just fetches the typescript file from there. Um, so that is also a bit annoying because URLs don't have semantic versioning. Mm-hmm . So you can't really resolve different patch releases. Mm-hmm . So you have to make sure that every dependency in your entire tree specifies the exact same patch version. So, because otherwise you run into this, this diamond problem of where like dependencies are duplicated and that is really annoying. And then we have the service code lip.dino.dev and they basically do like HTTP redirects. And that is all really ugly. Like Dino did not have a good weight of managing dependencies before JSR. Uh, so like, I mean, NPM is bad, but what Dino did at the beginning is like, that is just unacceptable. Like basically they could, instead of redirecting to the dependency we want to have whoever authors lived or Dino to dev, I didn't even know they could serve as arbitrary code. And we just import it and execute it. It's, it's really bad. Um, it'll be fixed. 
Yeah. I mean, I think once you have a working lockfile, it kind of works pretty well. Um, at least like when I think about applications, right. In libraries, I don't really care because it's not really useful to have it. But when I think about the applications I developed, then it like, once you have a lockfile, it works pretty well. What the problems are more that, uh, there's NPM and then there's yarn and then there's PNPM. And they all have different lockfiles that call differently. They're structured differently. And then I think the worst of them all is bun and they use like a binary lockfile, which is the worst idea because now you can't view the diff. Like you, you don't know what's going on and you can't even merge. It's that is like the dumbest idea ever. And, but, but I don't use bun. I don't know. Yeah. And so, but so the, this diversity of tools and they all have the different formats and then, you know, like you use your package manager and then you hop into another project and suddenly it's all different, different lockfiles, different things. Uh, that is more annoying than. Then some intrinsic behavior of how lockfiles work or like how NPM does them or something that usually works pretty much just fine. 

So not really. I mean, I find it interesting to have this opportunity. But I haven't really had a use case for it yet. I do open source and it's MIT license. So people can do whatever they want. And I don't have any warranty or like, it's just, you know, if, if it breaks for them, that's their problem. So I don't really feel like I need to ensure this. But, uh, sure. If you're a company and you want to make sure, you know, what code you're executing, then getting that is really, really useful. And having it built into the registry is like that saves so much time. Yeah. I see the value. 

Uh, I mean, I would sort of assume that an S-BOMS has more like information about it, that it also collects things like licensees and, and I don't know. Uh, but then again, you can technically derive from the S-BOMS. You can, you can technically derive these things from the lockfile too. You just have to like fetch the data for each entry individually. Well, it's probably an interesting idea to make these two equivalent. I don't know. Yeah. Yeah. No, I just wanted to know how you see.
But I mean, the lockfile also contains a list. So it's like, yeah, but it makes sense. Like, because the, the, if you have an S-BOMS, I would expect that there's also like licenses in there and maybe who also authors these packages and perhaps how long they've been updated. If you can link to CVEs. It would make sense to kind of integrate this. And that stuff really doesn't have anything to do with lockfiles. Right. So. It's, it's probably like the use cases are very different. 

There are a few things like, like, I don't know, even if your dependency management is very good, it's still advisable not to have many dependencies. So like, just the fact that other people can break your code. Is kind of annoying. Um, but it's like an inherently hard problem because you also don't want to reinvent the wheel all the time. So I don't, I don't really see any good approaches to improving the situation. Um, yeah, no, I think the short answer is no, not really, unfortunately. 

I think we're more strict with the regular dependencies than we are with dev dependencies. Like if you have a dev dependency and it breaks, then that's kind of annoying, but you can fix it and it won't affect your library consumers. Um, but still the, so what we do as library developers a lot is, uh, somebody reports a bug, we fix it on some branch. And then before we actually merge and release, we tell people, Hey, could you pull from this branch and check in your project? If that fixes your issue, just to confirm like end to end that this actually solved the problem. And so NPM has this feature called GitHub installs where you basically just install directly from a repository and then do like hashtag branch name. So, uh, what NPM does is actually download the repository contents and install all the dev dependencies and the regular dependencies, and then run prepare scripts. So basically we can execute any arbitrary code on developers machines if they do this, um, which is funny, but that's on them. And so, uh, that also means that development dependencies are installed regularly. And some people after we fixed the issue, like, I don't really know if they actually migrate back to a proper release version or if they keep on using the division that they installed from some branch. So that's why dev dependencies do get installed quite regularly on like in different production systems. And so already for that reason, it is still pretty important to make sure you don't have a lot of them. And that the ones you do have are very reliable. For example, we built our own compiler or build chain because we didn't want to use any other. And so we just make sure this one's maintained. And I, I think that's pretty much the only thing that everybody needs. Yeah. There might be, there might be other dependencies that I'm not aware of, but like we are still very careful with them, even though they are probably not as bad as normal dependencies. 

It always like it's, it's interesting because everybody who does software agrees that dependencies are necessary. And also everybody agrees that you don't want them. It's like this necessary evil. And it's somewhat comparable to build systems. They are also very painful and still you need them. And it's weird to me that we haven't come up with a good, like, like everybody who uses any programming language complains about the ways that this language manages dependencies. And I don't know. It's, it always kind of makes me a bit sad or triggers me to be like, Oh, this is annoying. We have to like do dependencies again. So, so that's, I don't know what you want to do with this observation, but it's just something that I noticed. 

Yeah. I do notice too, that a lot of people commit their lockfiles and I'm always like, yeah, but you're doing a library. This makes no sense. Like it's, do you know, it's making things worse? 

I don't know. I look into shrink-wrap. I've used it four years ago and then for some reason I decided that it's not worth it or that it's doing something that I didn't want to do but I forgot what it does. 


## P11

I guess it's about simplicity. And if I go back to when we set up Patito, the state of the art, if you looked away from poetry, was pretty much using Pip. And Pip was particularly slow at the time, to the unbearably slow. The dependency resolver was taking a long time. You had Conda as an alternative. Conda was better than Pip for a lot of stuff, but it was still extremely slow. And for a new user, having to set up a dependency, or setting up a virtual environment with the correct dependencies installed, could take a lot of time. So, lockfiles solve that problem by already presenting a resolved dependency graph, if you will. So, yeah, the main reason is to provide a faster way for the end user, the developer, I mean, to get up and running. 

Let me just get your questions up again so I can see them in the same way. Okay. So, definitely had version conflicts. But that's normally when I've been adding packages without... You know, sometimes I might be in an exploratory phase, and I'm looking to add different packages to my projects. And, again, I'm heavily influenced by Pip being very slow here. When I was using poetry, especially early on, I would, about each other, use either poetry to set up the environment, and then I might quickly Pip install something into the environment. And, traditionally, that broke things, because I think, at the time, Pip didn't have... It didn't do... By just running Pip install, let's say, SciPy or some other package, it didn't take into account the packages already existing in the environment installed by poetry. 
But, really, yes, I've faced issues with version conflicts. I haven't so much... I haven't experienced breaking updates with poetry. But I would have experienced the following same problem with poetry. 
And this is maybe a big issue with lockfiles in particular. However, this following problem I experienced with requirements of text files, where you've essentially pip exported, or pip freeze, so the same idea, saving your dependencies to a single file. So, there's a package called Polars, which is a fantastic data frame pandas replacer. But they were really, really aggressively pushing new updates. Like, they could add up to several versions per day on PyPy, on the package indexer. Now, that's great, to some sense, right? So, fantastic to get the new features immediately. But now and then, they broke stuff. And the way that they would handle that is by retracting. Essentially, going to PyPy and deleting. I can't remember what the term is. Pulling a version. Now, if you, at 9 a.m. in the morning, let's say Polars pushes a new version. At 9.30, you create a lockfile with that version locked. And at 10 a.m., they delete that version from PyPy. Well, then anybody who syncs using that lockfile hits a big issue. Because that version is no longer available. 
I mean, I experienced that a lot when we were using Docker images at my previous job. Because you would export the requirements to the text file. And then a week later, when you wanted to, or like a few days later, maybe, you would want to run some workflow. And you just hit an error. And why on earth, for your thinking about the error, you have the lockfile. 

So PyPy is not immutable as a package registry. Correct. Okay, that's interesting. Or at least, maybe I can add PyPy. It might be immutable in the sense that the version perhaps isn't gone. But it's flagged as retracted. And PyP at least doesn't. Or PyP respects that, you could say. 
But in Cargo, so if you have a lockfile, then even if it is yanked, it will still get that version, even if it is yanked. So with PyP, how does it work? Like, does it respect the lockfile and get the version, even if it is yanked? Or, like, does it just not get it? 
It reports that it cannot find it, or it's either cannot find it, or it reports it as yanked, and therefore doesn't install it. So you get an early error, which in some situations is desirable, right? So you don't want to push something which has, just imagine, like, worst cases, medical industry, where you have some life support system that suddenly does the wrong thing. You'd rather get the red error than it's killed a person. But on the other hand, it's not desirable either way. 

Yeah, okay. Yeah, so, I mean, I've specified dependencies in my PyProject.toml. Those are my direct dependencies. The lockfile is the resultant, the resolved dependency, right? So I typically use semantic versioning, and I pin to the latest major version. So then I simply run poetry updates, and I get the latest versions within breaking changes. Right. So when you run poetry updates, it updates the lockfile. 
how do you decide when you want the versions that's pinned in the lockfile or not? Oh, I see. Right. Okay. I mean, in general, then, I guess I, from time to time, run poetry updates with the intention of getting the latest and being in a mindset where I now have the surplus time and energy to handle breaking changes or issues that could arise from using the latest versions. Right. Right. Makes sense. Thanks. Yeah. Yeah. Yeah. And so when you commit the lockfiles, do you review the lockfiles when in code reviews? No, probably not. Like, I'm peripherally aware that the lockfile has changed. But I typically use GitHub. So in the GitHub UI, there's just too much noise for me to really go through and you could do a hard review. I will be much more likely to review any changes to a PyProject or toml or, you know, direct dependency file. So I'd say it's to some degree a weakness of lockfiles that they are so big. And that is perhaps a task better suited for a large language model or some other automated approach. 

Yeah. I mean, direct dependencies, again, I just go straight to the... No, I'd probably do a mix of things. I can't remember what the Poetry command is, but I would do the equivalent of PIP show and then the version package, given that I've activated the environment. I would do a lot of things to do with the contents or if I just want to control F through the lockfile is probably, like, my quick go-to. 

You could say it's on my mind, but at some point you just have to trust the process. Yeah, so I'm aware, like, there's too many dependencies for me to go through everything, for me to do code review on everything. So you rely on the existing community and you rely on that trust in that that there will be somebody out there who will have the time to review that particular PR or latest changes on Node or something. Security processes are, like, okay. They could be a lot better, I think. They could be a better... What do you call it? A verification of PyPy or similar. You hear about things in Node in the JavaScript community where really, well, things go very wrong. There haven't been any huge issues, at least that I'm aware of, in the Python community. So maybe we just haven't. Either they haven't detected them yet or we haven't had that big blow up which has led to real lockdown and better procedures. So it's on my mind. 

Do you fetch directly from PyPy or do you have your local <-> registry for those? Wow, that's so ridiculously on topic right now for me. Partially because I'm trying to debug a reason why Uv, or Uv, if you will, isn't able to get the metadata, the dependencies through, like, without downloading the whole wheel from the Windows, Azure, DevOps. So what you're getting at here is, like, having a feed hosted somewhere else. And maybe it also does upstreaming to PyPy, which is what ours does. So, yes, we absolutely do that. So we have a private tracker at work, which more or less, you could ask, why do we do this, right? Because we have, we essentially copy all the packages from PyPy onto our own one. And that's it. Okay. Well, why do we do that? I haven't had a clear answer to why we do that. 

Yeah. So I wanted to ask about Uv now. I mean, I'll just call it Uv. So why are you, like, considering Uv over poetry? It's just so ridiculously fast. Package resolution, virtual environment creation, installing. It's an order of magnitude faster than poetry and up to two orders of magnitude faster than PyP. Maybe not orders of magnitude in base 10, but at least in base three or four or five. It's really a lot faster. So that combined with ergonomics, I think that's the Uv, like, both the, since there's a few, there's basically a few things to, that a new user needs to know. So with Uv, you still have the pip syntax. You have the whole pip API directly in the command tool. So in Uv pip, but you also have this poetry, poetry inspired sync lock file project structure. And both of them work really well. So that's like the main reason I use it. It's nice that it's written in Rust because I'm like, I went into the source code of Uv today. And I know a bit of Rust. So like it gets all of the speed benefits from being written in Rust. And I could have gotten that speed benefit from being written in C++ or Java, or at least most of it. But I would not feel comfortable taking a look at the source code there. That's another benefit. 

So the, the lock file has this performance in this ergonomics, as you mentioned. So ergonomics is going to be good if you have a team of different people working on the same project. How large is your team or, um, how big is the. Yeah. Yeah. Great question. Um. So I think we have about 30 unique, uh, developers on the, the data science, uh, monorepo, uh, that we are working with now. Um. Maybe a bit more. Uh. So it's, it's, it's big enough. And it's a monorepo with multiple, you could call them workspaces. So, uh, projects with their own dependencies, including custom packages. so we are currently in the process of migrating everything from using a virtual environment based approach where you, we have our own Python script for setting that up, setting up Python, uh, setting up, uh, virtual environments doing a equivalent of pip sync. So we have synchronizing, uh, uh, three different requirements or text files. So we have a main. So like a, uh, uh, uh, main repo, uh, and, uh, uh, uh, type checking, pre commits, that sort of thing, uh, dependencies, uh, CI dependencies, as well as the per project dependencies. So you synchronize all those three into one requirements text and then install that. And that's something that Uv through the pip interface lets you do. We're currently migrating it to using Uv sync and project based completely. And that should make things a lot faster because then we have a lock file and everything is determined. 


## P10

Um, the major benefit I see is that my state is exactly the same when I pull down the code on a new computer. So say I'm working on, you know, the project on one machine, I commit all my code and I have all my dependencies exactly as I had to find them. And then on my other machine, when I go and I sync my repository, and it pulls all my data and it loads into my individual studio code. It has everything exactly as it was. So it's just great for getting everything into the perfect, the same exact state that I was working with before. Um, which you can't do just with PIP requirements, because it will pull down different versions unless you do a PIP freeze. Yeah, and freeze your requirements, which I don't like doing because it lists everything, including transitive dependencies, and not just the dependencies that I'm relying on, right? Like I only want to list out my top level dependencies in my PIProject.tomo file. And then I like that the lockfile, I don't have to look at it. It just lists every dependency automatically managed and synced wherever I decide to develop. So that's the primary benefit of the lockfile for me. 

Yeah, when I update it. Yeah, when I update it, I'll look at it. So when I do upgrades every so often, because I have like dependabot. And about watches the repository. And so every so often, I have to go and update my dependencies and update the lockfile. So that's the situation when I look at the lockfile is when I have to update it. But usually I don't have to touch it. Which is great. And it's, you know, it's like a huge JSON file. So it's kind of annoying to review the diff. And I don't like get a lot of value out of reviewing the diff, necessarily. But it's there, right? And it's good to have the history as well in the Git repo of every time I've upgraded. I like having that history for just for proving, oh, hey, I patched this bug in this dependency. Here's the commit. Here's the lockfile. I did, in fact, fix it, right? So it's good proof as well. 

I just like, okay, I'd updated that to a new version. I'd updated that to a new version. Okay, whatever. Right. And then I commit it. Right. But that's after obviously, I've updated and then I've tested to make sure the new dependencies are still working. I don't commit the new dependencies, if they're broken, right? Or at least I try not to. 

My understanding is poetry already does validation for the hashes. So when I go and I download, when I go and I sync my repository on a new machine, if the hash of the package doesn't match what's in my lockfile, it will give throw me a warning. Mm hmm. And it will fail. Because the package doesn't match what's in the lockfile.

 It is useful. Because then it tells me, oh, you changed your, you changed your dependencies, you need to re-sync. So it will say your lockfile's out of date. Mm hmm. And then it will make me regenerate my lockfile, which I appreciate.
 Although I am considering moving from poetry to UV, just for speed purposes. And UV also supports lockfiles. So it's pretty much a one to one comparison for me, at least. Mm hmm. But the big reason for poetry in the, originally was there wasn't another tool that let you build and release so easily. Mm hmm. That was my primary reason for using poetry in the first place. Right. Just because building and releasing is so easy with poetry. 

Yeah. So usually I'll, the only times the lockfile needs to change, the only times I'm dealing with poetry or is when, is if I need to upgrade something usually, or if I need to add a dependency. Um, so usually the first command I'll run, it depends on what I'm doing. Like if I'm adding a dependency because I need to use the dependency, I'll add it to the pyproject.tomo. Mm hmm. And then I'll sync. I'll do a full re-sync of the whole dependency tree with that new version. Mm hmm. And I'll update the lockfile at that point. And then I'll start developing using that new dependency.m. And then at the end, once I'm done with that feature, I'll commit the lockfile and the new code and everything. Mm hmm. But if I'm just updating a dependency because depend a bot said it's out of date or whatever, I'll go, I'll change the pyproject.tomo. I'll re-lock the lockfile, updating all the dependencies and everything. Right. And then I'll commit that. So you'll see the pyproject.tomo file will change and the poetry.lock file will change when I do that update. 

I looked into it. It just didn't seem to have the full feature set I was looking for. For I wanted something that could manage basically everything about my pyproject.tomo. Like I wanted my project to be managed fully. I didn't necessarily, I wasn't thinking about just dependencies. I was thinking about dependencies and I was thinking about building and I was thinking about testing and I was thinking about building. And I was thinking about building in support for running scripts with different tools and linters and things like that. So it was more of the fact that poetry could manage all of that for me. Right. And help me with all of that has a full project management tool instead of as just a dependency management tool. So, and I think that's why UV is also so powerful because it integrates so well with tools like Ruff for linting and hatchling for building. Right. So that's why I'm considering switching to UV because it offers all the same, the full, you know, experience of managing your project. Right. Mm-hmm. So, um, Pippi and V just, it just managed my pip part. Right. And it didn't really help me with the rest of it. Mm-hmm. Yeah, 

Yeah, I guess it's just, um, when I go to sync and rebuild the dependency, sometimes it's pretty slow. Mm-hmm. Um, I guess that's my usual problem with poetry is like, it's useful. It works. It, it doesn't, um, the only thing with poetry that I think is not great is that it doesn't match, it doesn't fully comply with the pip standards now. Like, it's, um, they, they've had a lot of, um, Python enhancement proposals that have been accepted for project management. That poetry, because poetry was developed before the enhancement proposals, poetry is doing things in a little bit non-standard way. Mm-hmm. And UV does things based on the standards. Right. And so that's why I'm thinking about moving just because poetry doesn't meet all the standards and it's not, and it's kind of slow. It's kind of slow. But it's still like poetry add, right? I use that all the time for adding a dependency, right? That's super great. It's, it's awesome. I, you know, it did, it did everything I needed to, to when I first needed a, a tool, right? Like build and publish. And, um, I guess the only other thing that I've experienced, okay, here's the bug that I've kind of experienced with poetry. Sometimes poetry will not find a package that you've just released because the cash is out of date. So if you go and try to get a certain version of a package, but it's not on Pi Pi yet, then you push it to Pi Pi, then you try to get it with poetry, it will fail because the cash says it doesn't exist in poetry. So you have to then clear the cash, the Pi Pi cash for it to do it. So it has some weird caching problems as well, that make it annoying to use when you're dealing because I have H-A-Y's API, right? I have that project. But then there's a dependency that I manage, which is Y's A-P-Y, which is also managed with poetry right now. Um, and, um, I have to update that one when I update the main one, usually in order to get all the features synced. And sometimes poetry fails to find the new package, which is weird. And UV does not have that problem. 


It's definitely beneficial to have a lock file for the purposes of Dependabot. Yeah. Dependabot is what I rely on heavily for making sure I'm dealing with supply chain security properly. Um, because, um, my releases are out of GitHub. So if I release, I can tag a commit, right? I can be a hundred percent sure with my lock file at this point that what I'm releasing is going to be okay. Right. But, um, obviously you want to be able to be up to date on what's new coming in. And the only way to do that successfully is to have good, it's basically like instead of having an S-bomb, right? I'm not scanning and generating an S-bomb. I'm relying on the lock file to kind of be my S-bomb so that Dependabot can do its job. Um, if I didn't have it, Dependabot would work worse, right? It just would not be as good. Um, um, but... So Dependabot will scan the lock file in order to determine the whole dependency tree, and then it wll warn you for CVEs even within transitive dependencies... Yeah. Okay. Exactly. Yeah. And then it will, it will tell me, it'll even write a PR for me. Yeah. And add it to my, it'll even update my lock file for me. 

So let's say you declare dependency to A. A uses B that uses C. Dependabot realizes that C is out of date. The current version of C in your lock file is out of date. But for you to update C, you need to update A and B, right? 
Um, not necessarily. Can you do the latest version of a transitive dependency without updating the direct one? Poetry update? I believe I've done it before. Okay. Interesting. There's a way. It's been forever. I think you can, you can do a poetry update on a sub-dependency. Cool. I believe. I'm not exactly sure. Yeah, because I just haven't come, the PRs have just worked, right? Yeah, yeah, yeah, sure, sure. That was cute. Yeah, yeah, yeah. How, uh, how I did it. So I did it. 

Yeah. Wondering if, if Dependabot would, would trace back to the direct dependency and, and ask you to update the whole path or, yeah. Let me check. Um, let me see if there's a history of that. Yeah. We can pull up, um, here, let me share my GitHub and we'll see. This is updating a main one, but I'll show you what it looks like. This is a PR that just got added recently to my Git repo. And you can see that the patch that it suggests is to the lock file. It's not to the primary. Yeah. Okay. Okay. Okay. So it updates all the hashes for me for the, um, for everything. Yeah. Even though it's a primary dependency, it just updates the lock file. That's interesting. Because my, um, it doesn't need to update the poetry, the pyproject.tomo file, right? Because my pyproject.tomo file lists this as an acceptable version. Right. Okay. Okay. So it updates the lock file for me. So I don't even have to, I don't have to think about it really. I just have to merge the pull request. Yeah. Yeah. Awesome. Yeah. Dependbot is good. Yeah. 

So having a lock file is like, have one, right? Have one in your project. If you're doing Python, there are security benefits. There are, you know, um, management benefits. There are, there are, you know, just ease of developer use benefits, right? There's, it takes away so much manual effort. So why not? Right. It's free. It's basically free. Security, right? Yeah. Yeah. 

Yeah I mean, if there's a merge conflict, I'll just go and I'll sync it and then I'll publish, I'll overwrite it. Right. If there's a merge conflict, I'm just going to overwrite it. Um, because, right, like the, the threat is not necessarily to me today. The threat is to me in the future from someone modifying a dependency and then me downloading it and it not being the same. Right. So usually I'll just overwrite the lockfile if there's a conflict. Right. 

Yeah. We, um, we basically require all of our projects to use lock files and to, it's like one of the findings that we'll, you know, if we're looking at a project and they're not using a lock file, we'll ding them for it and we'll tell them you need to utilize a tool like, you know, say they're in the NPM ecosystem, right? They should be using yarn and they should be using yarn with the lock file when they go to do their release build. Right. So you have to think about, are you using the lock file at every place where you need to be? So like in your release pipeline, you want to make sure you're using your lock file, right? You don't want to just be updating your whole dependency tree. You want to be using your lock file. So sometimes they'll have a lock file, but then when they're building the product, they'll forget to use the lock lock file and they'll update all their dependencies and then they'll install it. And it's like, no, no. You update your dependencies in the Git repo. You don't update your dependencies at release time. Yeah. Yeah. Yeah. So we will ding them. Basically, it's basically a requirement because it's that important for dependency management. I don't think you can do dependency management properly in any company without utilizing lock files for dependency management. If you don't have a hash of your dependency, you don't know what your dependency is. It's just that simple, right? You're flying blind. Yeah. Yeah. 

I think UV is a little bit better at this because UV creates a virtual environment in your projects directory, which is easier to manage. It's easier to delete and then recreate as well. Um, uh, poetry is kind of annoying in how it manages virtual environments. 

I like how Rust and Go do it. Rust and Go do a great job by default, right? Yeah. Yeah. NPM and Python. It's like, what are we doing? Yeah. It's all good old languages that have evolved, that try to evolve. Yeah. Yeah. Yeah. All the new languages, they kind of, they came with this knowledge already. 

## P12

Sure. Well, I mean, tool name's coming up on maybe like six years old at this point. So poetry maybe was out, although it wasn't nearly as popular. I think kind of like a year or two into tool name's kind of life, poetry got a lot more popular. I don't like poetry at all. I like the ergonomics of it. I dislike immensely. It was really slow. I know they've like made some changes to make it much more performant, but I just didn't see any need. 

I tend to not like using third-party stuff, any kind of dependency really, if I can avoid it, because it's just like one more thing to manage. And it seems like a lot of projects will end up having a lot of like technical debt around that and they don't keep up with that. And it's just like a nightmare.

So like sticking to just set up the pi and pip, but now pie project at Tamil made things pretty straightforward and pretty standard. And even today, like I have zero interest in moving to like UV, which I know a lot of people are hyped about UV now, but I'm like, I think that's a really bad idea because it's, you know, backed by a commercial company and that seems weird. So like, that's interesting to me. And then like conda was never an option because outside of data science, I don't see anyone using that anyway. So like, it was just like, okay, pivot is set up, set up the pie, super simple. So yeah. Yeah. 

So we saw that you had these like multiple requirements. So what was the reason that you chose to manage these like multiple files. Yeah. Yeah. I mean, I think like you see like requirements or requirements.dev or dash dev or whatever frequently. So I started with that and the, the original version of tool name was, or is still is entirely a hundred percent pure Python, no dependencies at all because it's basically a sub process wrapper around bin SSH. So it's, it's P expect kind of like, it's, it's very dumb at its core, but it has zero dependencies and it's tailored for specifically for network people to do this thing and make their life easy. And so having zero dependencies, I thought was like a pretty compelling differentiator, I guess, for tool name. That said, when I built it I used to work for Kurt Byers who created an Miko, which is like kind of the main SSH client for network people in Python land. And there was tool name I built like while trying to learn about that Miko and kind of make something I thought was a better boostrap basically. Net Miko, like a lot of other things just built on Paramiko. That's where the name comes from. And I thought, okay, well, that's cool. Using Paramiko is great. Cause then you don't have to have, you know, an SSH client on your, like if it's running a container or something. But I didn't want it to be a dependency unless you like opted in. So the way I set it up and maybe this was a bad idea. I don't really know, but it's worked for years, was to have a requirements file for each of these kind of like optional dependencies. And then those map one-to-one to the things that you can pip install optional extras. Like we, you know, pip install tool name, squirt brackets, Paramiko, and it installs tool name and the Paramiko dependency. There's obviously, like there's a folder and all that installs all of them as well. But so that's why they're that way. And I don't know that I've seen that in very many projects, but, um, so it's kind of non-standard, I guess, but it's worked out pretty well, I think. So. Right. Yeah. 

Yeah. Um, to be honest, for the most part, I don't look or care too much about transitive dependencies. Um, originally I started pinning things like super, like explicit, like explicit versions. Um, because before I wrote tool name, it was mostly writing like end user application, like applications that I would run in my domain. And then I wanted everything pinned. So it'd be very consistent. And then obviously writing a library is a very different thing where you don't want to constrain your users and you don't want to have like conflicts and stuff like that. So, uh, now, and for a long time now, tool name just pins the upper bound, um, you know, if a project uses semver, at least. Um, so we just pin to the upper bound that way. I don't care what the user uses. Cause they might use also at Miko and some other part of their thing. And that has a different pin for Terameco, for example. And then I just don't have to care about it that way. Um, and then as far as like transitive dependencies, really, there probably aren't actually that many, uh, with the exception of the Cisco pi ATS nonsense, which is a whole other barrel of monkeys. Um, but like text FSM is pure Python. There's nothing there. Per Miko I think is cryptography and pure Python, uh, async SSH is pure Python. So like all of these dependencies, like transitive was really not really a concern or a problem. So that's why no lock file. Like, yeah, I still wouldn't use poetry or UV or whatever, but like, obviously I could pick lock, but like, again, for a library, I don't think generally that would be something that you would do. 

Like with just with the requirements files, like, uh, yeah. Like to add a new dependency. Yeah. Yeah. I mean, I honestly, that hasn't happened in like maybe, maybe a long time. Um, there was an, another parser called TTP, this guy in Australia made, uh, which is like a regex style kind of parser for unstructured data. Uh, but again, that was like, so yes, added a dependency or requirements file for that, but that was pretty much just one dependency. I don't think he has any dependencies other than standard library stuff in that also. So it's just like, yeah, okay. Both done. But I, that's been the only change to dependencies in like a very long time for tool name. So it's been pretty static on that front. Right. And so, uh, you don't like, uh, how that much of, um, um, uh, like requirement for deterministic builds, I guess, like, because, uh, yeah. Okay. So it has never like, um, I think like if you were a user of tool name and you're using it in your thing, like, yeah, sure. Great. Use a lock file, be, you know, build it. You could make your end thing deterministic, but as a library author, for me, it's, that's not really my problem. Right. I mean, within reason, you know what I mean? 

No, I mean, no, to be honest, no. I mean like I, for like work things, you know, S-Bomb comes up a lot, especially in like kind of CNCF landscape. Um, personally, I don't really care. Like, I don't know that security is not really been a concern for me. Um, I mean, I trust PIP within reason, you know, I mean, but not like any of this is at least as a library author, like, I'm like, well, you can check if you want. Um, I'm, you know, I pin to like reasonable dependencies. I'm obviously not adding dependencies that are, uh, untrustworthy. And, you know, if something upstream happened, like this is a side project for me. So like someone wants to pay me to like spend a bunch of time caring about that kind of thing. It's probably not gonna happen. 

I pretty much don't care at all about windows. So that rules out like a really big chunk of user problems and dependencies. I just like, I don't care, like use something else. It's not, I don't windows and I want windows. And so that's the end of that story. So that helps a lot with that. Um, yeah, I mean, Python could be a pain for dependencies for sure. Um, but again, in the library with like, just set upper bounds on the requirements, it's been like from a library perspective, it's been fine. The only, uh, dependency in tool name that's a big pain is the SSH2. Um, because it's a C library on the backend, obviously. Um, and that project has been not super maintained. The maintainer kind of like disappeared. And so it was like, there was no wheels for three, 10, 11, and 12 and 13. And now they're all there. Like he came back from wherever he or she was in the ether. I don't know. Um, so that's been a little bit annoying, but it's an optional dependency, right? So like core tool name, it's like, it works. I don't, you can choose to use that or not. And that's up to you as an end user. And you could, because Python, you could patch it. And there were people that were forking it and, you know, so like, okay, go crazy if you want, but I'm not gonna, um, deal with that, I guess. But yeah, that's probably been the only other major requirement related kind of challenge.

How do you, what methods or what are the processes that you put in place yourself and the group of people working with you to mitigate? I, yeah, I take that pretty, you know, as serious as I can for like a side project, obviously. And I've like in the past, I've used tool name for work. So obviously, over time, it's built been built up of like kind of different phases of my involvement or whatever. But I mean, unit tests, there's a zillion, zillion, zillion unit tests in tool name. With a lot of like kind of stuff that you couldn't get away with in Go or Zig, which is nice, which is like monkey patching some stuff to test behavior around other things without having to like have actual IO, because obviously everything depends on IO at the end of the day for tool name. Yeah. And then a good friend of mine is kind of the main maintainer of project called Container Lab, which is a Go project that manages network topologies using containers, or sticking VMs in containers, that's a whole other tangent. But to spin them up and create reusable, shareable network topologies, and just run it in Docker. And then I built another project called Kubernetes that takes that and deploys it in Kubernetes, so you can actually have some horizontal scale. So I use that to spin up virtual topologies in kind of a functional or an end to end test as well. So then there's obviously no patching, there's no nonsense, like you get a real like, if that works, you have a pretty high degree of confidence that things are going to do what they do. So yeah, I mean, I guess just that and like really, really, really overly probably aggressive linting. Every project I do, I just like, it just seems insane to me that more projects don't lean into that. Because it's like people that are like really weirdly passionate about line lengths or something, or like, you know, variable shadowing or whatever. And it's like, why, like, you should just use, like, the collective brain trust from the community and like, you know, take advantage of that. So tool name and like all of my projects have a ton of linting, which, you know, that's not going to fix like correctness of functionality necessarily, but I think it certainly helps. And it certainly keeps things consistent. And like, I mean, for six years old project, and this was like my first like real, real Python library. I look back on it. And I'm like, yeah, I would have done this a ton differently. Of course, like, obviously, we, you know, what I wrote tomorrow, I would write differently today, like, yeah. But I'm reasonably proud of it. And it's, and it's pretty consistent. And that's helped keep it on the rails, I guess. Okay, so no dependencies, millions of unit tests, millions of linters, and integration tests with those virtual topologies. Yeah, basically. 

Yeah. So I will ask a few questions about the goal version of your projects, like mainly about Go. And yeah, so like, how do you compare managing dependencies with Go compared to pip? Yeah. I mean, I think probably most people would say goes better. I would agree. Everything's, you know, explicitly pinned, you know, go. Go. I don't know. It's just, it's fairly easy. Like I could see there being some problems and like really, really massive projects, but even like I have Kubernetes as a dependency in Clabernetis. And I've never had really more than a handful of times where I've had to be like, you know, kind of mess with pins and stuff to work around like a transitive dependency problem. Just the way, I don't know what magic Go does to make that happen. Like I'm not like deep in the weeds on that or anything. But from my perspective, I basically go get, I set it to a pin or a hash or a release, you know, whatever. And then it just works. And I don't think about it. And, and it's, it's interesting to go back to like, since I've been writing a lot of Zig lately and I've been seeing all the C stuff and you go back and you're like lines of code that are 20 years old and get blame or whatever. And dependencies that haven't been updated in years. And it just works because it's all just very much static, I guess. Maybe that's not the right word, but it's much more explicit. Maybe it's the right word. And so that's cool. I like, I like Go a lot. Python is still great, of course. But yeah, from a dependency management perspective, I think Go is just an easier, it's just easier. It works better. And, and, you know, it's a newer language and lessons learned, like whatever, like obviously there's reasons for that, but yeah.

It's basically almost the same because go.mod is also kind of, um, very minimalistic. Very minimalistic, but with go.mod, you do have the transit dependency. Like, uh, Yeah. I, for me, like, to be honest, if I, if I see like the, whatever in my, I use go land, jet brain stuff. And if I, if I see the transitive or whatever, the, the, that inlay that they put there to say that it's, you know, not my dependency indirect, I guess is what it says. If I see that, I just ignore it and then everything just works. Oh, I actually have to think about this. And like, maybe I have to pin back to a different version or, you know, this, that, or the other, um, like ever with go. So I guess you get all of the benefits of like, you know, pip lock file or poetry lock file or whatever with go without having to think about it. And I feel like in Python, have to think about it. Um, so yeah, I don't know. It's just to me, it's just a kind of a better bootstrap, I guess. 

 I mean, for the most part, like, um, there's a CLI tool go mod upgrade and it's just like, like for Python, like every time I cut a release, I'll just go to PI PI. And because I have like five dependencies, this is very easy to do. Obviously if you had more of this would suck. Uh, but I just like, look, do, Oh, is there a new major release? And if there is then before I cut a release, I'll just be like, okay. Um, update the higher pin and run all the tests. And if it works, like it's probably fine. And then I just say, okay. Um, go mod upgrade does the same thing. It just checks all the upstream source. So then it'll be like, Hey, you have these, these pins updated. And it'll be like red if it's a major, like summer bump or yellow for, you know, or green for a patch. And then you just say yes or no. And then it just updates it. And then you just run your tests again and everything's cool. Then you're pretty much good to go. So, um, yeah. Same process, just slightly easier and go. 


## P14

So yeah. So why did you decide to select Go for your project? Yeah. Good question. It goes back a minute. Like, you know, I've been working in like, they call it like platform engineering, which some companies call platform, other companies call it core infrastructure. It's like the first layer on top of the actual infrastructure. And that's where I've been working. So, and that is often in Go, you know, Kubernetes is written in Go. Sometimes I need to extend the Kubernetes API a little bit. And for that, you always use Go because then I can just import the native libraries. And for that reason, it's like the language I'm like best in. But even if I wasn't, you know, I would only write CLIs really in two languages. I would use either Python or Go. But I'm so used to working with type safe languages that I went with Go. But a lot of people do data analysis, machine learning stuff in Python. And if that's, if that's a language you're really good at, I understand that you go with Python instead. 

Before I forget, another reason I chose Go is because you create binaries in Go. And it's, once I'm, I can make like, on my local machine, for example, I can make binaries for like, 10 different types of computers, right? And then I can just distribute it very easily. Whereas with Python, you may be tied to a certain Python version. Because Python is not a binary, you have to interpret like a script. You have to just read it with an interpreter, right? So that was another reason for me to pick Go. Sorry. So yeah. So yeah. Go has a native dependency management tool. It's called like Go Mod. Gradle or NPM or others that you mentioned, they are not native to the language, which is not a, not a big deal, right? It's just, it's just a gotcha. 
Certain things I like about Gradle. Certain things I like about Gradle that Go doesn't have. What I like about Gradle is you can do package level dependency management. So you can say like, like within your, within your tree of packages in Java, you can have Gradle files and say like, hey, this package needs this specific Java package I'm working on. It needs these dependencies. And this other package needs these dependencies. And then when you browse through the code, it's kind of nice to see like, oh, this is this and this is that. And you can, you cannot do that with Go. With Go is just for the entire program. But it's very, very simple. You know, there's a Go mod file that shows direct dependencies, indirect dependencies and your Go version. And, and then, and you have like a Go sum file that shows your, your, what do you call it? Check sums. Yeah. Yeah. So, yeah. So, yeah. So, yeah, it's very, very simple. It doesn't have much of a learning curve, such as like Maven or, or Gradle. I mean, Gradle is, that's his own, they use it groovy, right? So you have to like learn that language. So, yeah, I mean, I think that's, that's, that's, that's, that's tricky. Like, with NPM, I mean, JavaScript is, is a very fast changing language. So there are a lot of like breaking dependencies when you update JavaScript code. But that's not NPM's fault. So, yeah, I have nothing against NPM. Just JavaScript is, it's hard to, it's, it's work to maintain. What else did I want to say? I'm like rambling, I feel like. Wait, what was your other question? Sorry. Yeah, no, you answered the question. So that's fine. If you have, if you can remember something else later, then you can also like say later. 

I think these languages have like different like use cases. Um, so like I said, I'm like, uh, I always work close to the infrastructure and, and for that go is like a great language because, because the space is dominated by Kubernetes, which is written in go. Um, I, I do think languages like go can get a little overused. And people start coding like their business logic in go for that. I actually have a strong path preference for the Kotlin language, which, and with Kotlin, you can use a Gradle as the package manager. So I like both. It's just for different kinds of work. Here's the thing, you know, you start a company, you may have only 40, 50 engineers and you're not ready to make this split in like two languages. So you have to go with one, with one of them, you know, so, um, you may, uh, use Gradle where, oh, sorry. You may use Kotlin where you should be using go and the other way around. Um, I, I have no strong preference between the two package managers. I think they're both great. Um, I, I, I like Gradle a lot more than, than Maven. Um, because with Maven is like kind of an, you need to create like a pom file, which is an XML, which is just horrible to read. And, um, I believe Gradle is just a wrapper around, around that, around Maven. I could be wrong. Um, which, which makes it all a lot cleaner in my opinion. Um, not now, now that I'm talking about this, I wanted to mention the previous question. Uh, I always vendor my dependencies and, um, that means that I package my software. I include the dependencies in my software distribution. Um, and this is something I, I learned like while I was working at <-> it's, um, you, uh, want to be able to, uh, reproduce your builds. And the only way to make sure that you can reproduce your builds is to have your dependencies packaged. Um, and what I learned at pivotal at <->, I worked with a lot of large banks, such as like JPMC. I learned that, um, they often don't even have an internet connection for their builds, you know? So NPM is not allowed to reach out to the internet. So you need to vendor all your dependencies. Um, I don't, I don't know if that's very relevant for you, but, um, that's an important part of my workflow. Um, Yeah. Um, I, I, so you do this so that you can rerun the build on an air gap machine. Yeah. And, and one way of doing it is to vendor them all. Uh, another way of doing it is to cache all dependencies, uh, high in your build system, like on a higher level. So if I write a software program and you, and you write a program, we can fetch the dependencies from the same, uh, cache that we host ourselves. So, um, that, that is another pattern I've seen. Um, but, but, but to be honest with you, like, even at like Spotify scale, I wouldn't do that. We just packaged it in the program. 

Uh, I think Go when, I think the dependency manager wasn't available in the first versions, right? So there was this other dependency manager, I think it was called Deb or something. D-E-B, I could be wrong. And, um, at some point Go started using their own native dependency management system. Um, one big breaking change I have felt was, um, Um, a long time ago, like five, six years ago. It was, um, the HTTP mock. So I always do test driven development. So I write my tests first and, um, the HTTP mock library, I think it was called, it was mocking an HTTP server. Um, that went away. It was deprecated by the, um, by the person who created that library. And, and that was a pain because then I had to start doing it all. Um, we had to start writing our own mocks. So that was a painful transition. Um, but Go is a fairly stable, the language itself is always backwards compatible, right? Just like Java. Um, every good language should be backwards compatible in my opinion. Um, but, um, yeah, the libraries of course are just maintained by, by people, by anyone, right? It's open to can be open source. Um, so in the libraries itself, I have seen very little breaking changes in, in Go. And, but I have to say though, um, I stay very close to the metal. Uh, sorry. I stay very close to the standard library in, in Go. And that is, um, that is a pattern that most Go engineers follow. So in Java and JavaScript, sorry, you will use libraries for everything. You know, you will use libraries for, for padding. Like I need like two white spaces in front of my text that you will just import some library for that. With, with Go, you, you try not to import too many libraries because you want to keep your binary small. Um, so because you're not using that many binaries, there are also not that many breaking changes. If that makes sense. 

Yeah, definitely. I, I blow that whole file away once in a while. Let's just get rid of it. Okay. Oh, um, another, sorry. Another thing I wanted to mention about dependency management. What I usually do is, um, I run some scripts. Like, uh, you can, you, you can do it like in GitHub. You can use like a dependent button, dependent button to check your dependencies, but you can also just create your own script and, and, and just run it in like a crontab or wherever you want. Uh, and I just check if there are new dependencies and then, um, I will bump the dependencies and then I will run all of the tests. So I got a unit test, integration tests, uh, and contract tests. And so I rerun all of the tests and if they're still green, I will just, uh, merge into the main branch. So this is, this is a pattern that you will see a lot like, um, in like larger, larger companies just to, um, automatically update the dependencies, you know, so you don't have to do it by hand. Um, and this is like, uh, this is why you like JavaScript is a pain because you do text, your tests actually will go red, like once every couple of weeks. Yeah. So you actively proactively bump all the dependencies. So you have a script that will go through your mod file. Well, I can show it to you if you want. 

Yeah so i have all these scripts you know uh ls scripts and and i make all these scripts available in um in my readme you know uh sorry in my make file so so you can just do make tests make commit make country yeah you can do like make unit or something yeah we'll like run all the all the unit tests um or make integration etc etc um scripts so then i also have like make update devs so update devs is uh kind of scripts update devs it's just it does a go mod here this is the command to fetch all the dependencies go get you and then just recursively fetch all the new dependencies then um go mod fender then i fender them because i'm in vendor mode and then i tidy them up so i just clean them up and then um oh this this just updates the dependencies and then i usually do after this i do like make all tests uh so i i i'm doing this by hand but i also have a dependent running every every 24 hours on on github so um chat um it is old oh yeah dot github yeah dot github uh dependent but that yaml so also just have a dependent but running you know and it's um for the goal mod ecosystem it checks uh at 1am it will check if there are new dependencies out and then it will just um it will just um pull them in pull the new dependencies in and we'll do make a pr and then um on a pr my tests will fire off automatically in in github so um all my github tests so it will run this make all tests thing in github and um if this is green then then i'll like merge it in this doesn't what i just explained this this doesn't scale though for uh for like spotify so at a larger at a larger company you will just um run a process that that checks this for you you know like uh you can uh you can create like a kubernetes job that does it um that runs once a day or you can use a lot of different kinds of cicd software you know you can use uh concourse which is written in it's maintained by canadians um you can use um what you call them what are the what are the large ones um what is the dependency manager that you um that is atomic with your software so you have like a hidden folder in your in your uh repository for it um i forgot um i forgot anyway uh you can use github you can use uh other kinds of things to to just run this automatically.

Yeah i mean you definitely write that the go sum gets corrupted once in a while that that's the only like pain like um but then it's very easy to fix but um yeah it would be it would be nice if if go if if the go folks could do something about that yeah when it comes to the commands i run most you can check my scripts scripts directory of that repo of my cli


## P15

I personally find NPM very confusing because it's not very transparent. Like you have to, you just say NPM install and like a wall of text goes down and you have a gazillion packages installed and you don't really know what happens and why those are installed. Compare that to Go, which is, in my opinion, very open and very clear on what dependencies you are using. You have like the direct dependencies of your project and you see then automatically. These are like indirect imports in your Go.mod file, which to me is very good because you know, okay, this is why I have this in my code and this is why I need this. And you can always, the Go.mod tooling allows you to even identify which packages install Y. Like you can say Go.mod, I think Y or something like that. And it will tell you, okay, this is basically the reason why you need this dependency in your code. And then you can decide, okay, do I even need that? Maybe I built my own. Do I actually want this? And you can like actually see and check the dependency of your code, which is very transparent in my opinion. 

I mean, because like, especially if you have a good IDE, with a language server integration, everything basically works by itself. You just enter the package in your IDE. It will do the Go.mod tidy and download all the files that are needed for the package and you are good to go. There's no like, put it in your package.json file, market s dependency, market s development dependency. Just say, I want to use this in my code, go more tidy, and you're done. It will download it. And that's very convenient in my opinion. 

So whenever you add a new dependency to your Go mod file, it will download the latest version, which is good. I mean, that usually has the best results if you have like issues. But it uses semver and semantic versioning in the Go mod. So whenever I need different version, like an older version, I just add that specific version to my Go mod and everything will be good. Of course, if then I have another dependency that relies on like the same package and a newer version, you might run into trouble there. But let's be honest, I usually try to be on top of the latest version of the modules I'm using. With Go mail, I'm currently in a situation where it's a little bit complicated. We are using code from the Go extended library, the extended standard library. And they recently introduced a new version to that package that we are using, which requires us to have Go 1.23 as the minimum version, which clashes with our compatibility of we support the last four versions. So that's kind of an issue. But the good thing again is we are not using the actual code of that updated package. So it's not really related to us. But at some points, we will run into the issue that maybe the code that we are using is has a vulnerability or something and needs updating. And then you are forced to update it to like not have a vulnerability in your dependencies, right? So that is the only downside. Like if the package maintainer decides, okay, we need to support this in this version now, then you are bound to it. And yeah, that can be an issue. 

No. I think Go modules is a standard since Go 117 or even earlier. So that's five years, six years ago. So I never really ran into issues because I started Go development in 2020-ish. So I've never run into a non-Go mod version of Go. So I honestly have no... I of course see the vendoring packages like in older packages, but I never really actually had my own experience with that. I never run into any issues there. 

Yeah, as I said, only the... It's not really like a real issue for us right now in Go mail, for example, since I said we are not using the part of the package that has to be updated. So we can run the older version until it's like... until we are forced. In other packages, I had that once or in another piece of software I was writing, I had this once where they updated to v2. But the good thing is like it's kind of common in the Go community that once you go to a version two, you have like a new branch. And so you have like kind of a compatibility with the the old version of the v1 branch that you're using. I wanted to use the v2, so I had to change some code. I was forced to do that. But if I stayed with the v1, I would have not any issues. And I think that's very nice about... it's more a community like standard, I would say. Like people usually try to have not breaking changes unless you really have like issues that need to be fixed. And especially like in the Go standard library, Go has this compatibility promise basically that any update they will do, it will never break your code. And so far, I've never run into issues where the actual Go update broke any code on my end, which is good. Yeah. It's really nice. 

I use the Go mod tooling. So that always uses the proxy. So I have no idea about the amount of downloads and stuff like that. I've run into an issue like with the checksum myself because I made an issue for myself. I basically released a new version of the code. And then I found a bug, which was only one line of code. And I fixed it and immediately released the same version number again, which broke the checksum because somebody already pulled that version and the Go proxy already had that basically broken release downloaded and stored in its proxy cache. So people were like opening issues that they had like checksum issues. And yeah, that was on my end. I didn't expect people like to be that fast on the update, like within 15 minutes, having the new version downloaded. But apparently I was wrong. So I released the new version with the same code, just with the new version of man that fixed it then. So, but yeah, that can be an issue. I mean, it's good on the one hand that you will always, like from a security perspective, you can make sure that you will always get the code that is basically stored on the proxy. And if you don't have that, something is wrong and the checksum will give you an error. But then again, it can cause issues if you like want to re-release the same version. And yeah, it's like convenience versus security is always like the... Yeah. Yeah. Even with other package managers, most of the package managers, they are immutable. So... Yeah, exactly. ...release. Yeah. 

No, I even let the old release in there and I just released a new version number. So Go will always take the latest, the mod tooling will always take the latest version that's marked as latest on GitHub. And so people just had to reload it again and they would get the latest version in that case. 

Yes and no. I rarely do it manually. I usually use like automated tools for that, like GitHub Dependabot, for example. And like if I know there's a new version and it has a feature that I'm looking for, I force it and use the Go mod tooling definitely. But most of the time, I just get like an automated pull request from Dependabot or something similar and then I will like get the newest version automatically pulled into the project. 

Yeah, 100% yes. Yeah, right. Yeah. Usually like it depends on the project. Most of my projects have very few dependencies, unless it's like a private repository that I only work on myself. But the like more public projects, I always check for dependency and ask, do we really need this dependency? Is it... For example, we had one in the I have a have a been pwned library and it needed md4, which is not part of the... not part of the standard library of Go, but it's only one Go file in the extended library. So I said, okay, let's not just import this whole dependency just for one Go file. Let's just import the code over because it's probably not changed anyways. And yeah, so usually I check the dependencies and basically question is this dependency needed or not. 

To be honest, I really don't care where the checksum is stored. It makes it clean to have it separated because I only have to put it into the Go mod file and then the checksum will be generated automatically in the some file. So that's like having a look at your Go mod file is much cleaner than, for example, looking at the J package JSON, which is huge sometimes. And compared to Go, you probably have three or four lines of code in there, which is much easier to identify. But I honestly do not really care about where the checksum is stored. It's nice, like if you have no dependencies, you don't need a some file. So that's a safe one file. But I think that's just, that's not really needed, to be honest. 

I would say yes. I've never run into a case where I said, okay, this dependency, I don't, I want to exclude it because in that, I wouldn't even import it if I don't want, like if I want to exclude it, right? Yeah, indirectly, that is true. But I'm, I'm, I think that the Go community is there a little bit more scrutin on that because I don't see so many like indirect dependencies in my corporate, maybe that's only me. But usually my Go mod files are very like, they do not go over one page of my screen. So maybe that's a Go community thing. I don't know. Yeah, that could be because like, it could be a community because like NPM is the one that has lots of dependencies, like even for very small things, they have libraries. So it becomes huge. 

## P5


NPM run all two, yeah, was like the other one you mentioned, which just crossed, I think, like 90,000 public dependents on GitHub. Yay, whoopee for me. 

I do run into dependency, like issues due to dependencies, due to version conflicts and breaking updates. I run into them, ironically, more on projects that I work on that use lock files than I do on projects that don't use lock files, which was kind of like the theme of the other questions. And we can get into like why I think maybe that's the case. Yeah. But I think like when they're used on projects without lock files, there's a term called like in range breaking changes. And so in NPM, you in your package JSON, you define your dependencies with the server range for a bunch of reasons, but mainly like, like, just because like, you know, allows publish module authors to publish patches and have them get out a little bit easier. When when when I what when you have a working like project with a lock file, it like will generally continue to work for the most part, there are edge cases. But like, the situation I see most frequently is like somebody will add or remove or update something in a project with a lock file. And this will cause a partial regeneration of the dependency tree. And then this is where like, weird permutations of different versions, uniquely are generated in this project with this unique lock file. And that's kind of where I see the most likes most like most like, most like common places where I get like, you know, something's broken down because of a dependency update. And then the other thing too, is when you have a dependent bot running on a project with a lock file, it has to update the top level dependencies as well as a lock file. And those circumstances that I find a lot of like, like, there will just be something weird about the lock file and how it resolved over time. And that's where I see the defects sneak in. 

On projects without lock files, you're constantly generating the ideal state at a given moment. And oftentimes, like, either you'll just miss the defect because it will be introduced for a short period of time and then published past like with a fix. And so you just won't even notice it just because you're always resolving the ideal tree, like at a given moment, like that tends to be a better resolution than whatever state your lock file is accumulated over six months or whatever.

Yeah, so there was a there was a project called Greenkeeper a while ago, like a few years ago, that was like dependent bot, it was like you add it to your repo and it would scan it every day for updates. And it had this unique feature where every time a direct dependency had an in range version, it would open a branch with the updated version and run your tests. And it would then open an issue if the test started failing within range. And that was like an in range breaking change. In practice, I found those would happen maybe like once or twice a year across all the packages I maintain as a pretty rough process. It was a pretty rare, rare, you know, 12,000 line lock file, you can, you can look at the diff there and it's not a big deal. I actually wrote a blog post I linked I sent to you in my email just now. It's not it's it's it's it's it's a rhetorical funny post where I make fun of lockfiles and their issues. I haven't published it yet. I want to kind of soften it up more, but you're free to read it. 

Hopefully I don't make a fool of myself, And I got I'm going to change the title. The title is just a joke. The joke was lock files are trash. You should throw them out and regenerate them periodically. But obviously, you know, I'm just trying to be I'm trying to be rhetorical here just to like, you know, for clicks or whatever, but I haven't published yet. I'm going to change the title. It's too mean. They have their use. But I've had this idea like this will never happen. But like if I were node king or whatever, like I would switch lock files to lock dates with overrides. And so rather than like locking down like everything out of specific random permutation of versions of whatever happens to be in our modules at a given time, like it would be an ideal tree at a given date. And PM has this tool called like the net like or what's the flag called the dash dash before flag that says install this package JSON as if it were this date. And because NPM is mutable, mostly immutable package registry like that is a reproducible process. There are variances, but, you know, the tar balls will never change once they're up. They might get deleted, but due to policy violations or whatever. But like I think that would be an easier thing to do. And then like to run your updates, you would just update your date to like today and it would like re-resolve the tree. And then, you know, tools around inspecting what actually happens. That would be cool. But that will never happen because NPM is like abandoned where at this point <-> isn't like doing anything with it. So maybe the next package manager will do that. 

If you specify a version that was published beyond that date, it doesn't work. So there's still like pickups. But like it would be cool if you could like if there are more robust tools around like immutable registries and dates. I think that is like a really cool pattern that's like simple enough for humans to like work on and just kind of save us from the insanity that is these machine generated lock files, which are just, you know, they're human readable, but they're not because they're so big, you know? 

Yeah, for the record, I'm a I'm a weird person like I people love lock files. They love committing to get and they love accumulating lots of state in them. Like I'm I'm a weirdo in that sense. Like so I'm definitely a fringe opinion here. Yeah, but you have very I mean, working at <-> It's basically you this company is caring a lot about dependencies. There's I'm surprised there's not more tools out there. I mean, like that there's definitely a lot of like open source insights that we're trying to bring to the product. It's still really of a hard problem to solve. And like, you know, we're not perfect either. So about but I think there's still a lot we could do, I think, from a product perspective, for sure. For example, just just like, you know, better UX around like the data we have and things like that. Like just getting that data out in a presentable way is a challenge. And our most of our customers are enterprise at this point. And so like we're catering to their needs versus like open source. It's still a priority, but like, you know, open. So like I said, like open source doesn't pay for anything. Yeah, like, you know, you don't get you don't get that big of a say other than volunteer volunteers. So yeah, but so I guess those companies that you work for that <-> is is helping. They do have those large dependency trees. And I mean, yeah, as you said. Yeah, I mean, it's like paid staff with various like experience levels, a lot of inherited code bases. And like that's and that's the sort of substrate where like supply chain attacks like do best. Right. It's like people go to work, they get paid to go to work and in some sense, like care less about or just no less just because like the company is trying to preserve the code base and bringing people on and off. Like that's like where things are like information is lost. Like when I work on NPM run all like I know everything that's happening in that project that I have for the last few years. Like so sneaking stuff into that project is a little bit more difficult without, you know, me or other people noticing very quickly. Whereas like in big companies, it's like you have to have a robot doing it because people do sort of internal arrangements that these companies just can't manage. 

I mean, <-> tried to do that a little bit like we're focusing more on like malware detection now. But like earlier on, we were like, we had a bunch of like categories of tests that were like, you know, is this popular? Like, not that that's a good thing. But like, at least it's not a, you know, not being popular isn't a bad thing. But like, you know, sometimes you just want the default choice, whatever. Or like, does it have like network access? And if it's like a template language, like why is it doing network access? You know, things like that. We had those types of like categorizations. It's just challenging to like identify that in the tree of a specific dependency. Like the big challenge there is just like, you know, direct dependency analysis is very simple. But then the actual tree at any given point in time, like that's the challenge of like presenting a lot of information. It's always changing. It's like shifting sands, you know? Yeah. In terms of like picking stuff, like I would say like open source is like kind of a pseudo social network. And it's like the like picking dependencies by friends and colleagues is a better strategy than like technical analysis in terms of like overall simplicity. But it's also like soft and gooey and like hard to like quantify, you know, but like picking stuff by people who work similar to you and that, you know, and they know who you know. And like that's a great proxy for like, I don't know, picking things you like, you know, quality without defects or whatever. Maybe, I don't know, maybe the defect card not, but at least like, you know, you can talk to the person if there's a problem. And so we've so along our research, reading papers, talking to people. 

We've seen a few times this idea that you might as well copy paste the little snippet of code instead of fetching an external package and then just get rid of the whole dependency thing. I think what's yeah, so this is this is a problem that's going on right now. Like, like, basically, Node.js was was a was a revolution in the JavaScript. It's a subset ecosystem and then a larger JavaScript ecosystem. It was a massive revolution. And two fronts, it brought like JavaScript to like a Unix process, basically, which was like, you know, why didn't anyone do this before? You know, you know, and also it fixed a lot of like design problems around package managers, namely, like, I would say the biggest defender would probably be PyPy, where like they have this inverted package manager where like you have to simulate a system to install packages and they can't have recursive dependencies. So like it like it fixed a bunch of like DX around dependency management. And the third thing it did was versioned project dependencies. So it was like, like your project could define like development tools at a version that would install locally easily. And then you could call them through the scripts field and it would be at the version of your project needs.
And so like you wouldn't have always like system wide dependencies everywhere. And so this was like this massive revolution that I got to hop off here.

I mean, I think it's like this is this insane explosion of like packages due to this and then all of this was done on top of like the rise of GitHub. So like this very accessible platform and like new SVN tools. So like, like, you know, NPM shot up to like 8 million packages like right away. And all the people who were like early on this, like a lot of like early maintainer people, it attracted a lot of people who were like excited about getting like users on software. So like get a lot of early adopters. And then a lot of people realize they didn't really have it in them to like maintain like dependencies for like other people. And then they didn't have the attitude to just be like, this is my thing. I'm not going to work for you or whatever. And so they burned out and like left. And so like now there's always like projects that everyone's working on where there's always like dead dependencies that nobody is maintaining. And so I think there's a lot of like feeling that there are people are like subject to their dependencies versus like rather than being this thing that they can pick and choose and fork and throw out. Right. And so like the like the push right now is like, fuck dependencies, like, let's get rid of them. Like, let's this is the you know, these are the worst things ever. And it's like, it's just a tool like it's a very powerful tool. Like you're not using it right. Like, why get rid of this capability? Like, you know, why do you want to rewrite like like the things that I've carried between jobs have been like modules that I've published. And those have like a longer impact on my like developer career than like little internal projects at this point. So I think it's I think it's a shame that people are like down dependencies. I think people don't. On top of the fact that there's like TypeScript and like people are working in React, there's always like layers above everything now. So like they never get down to just the node layer and like working it. And that's the joy of working. It's kind of a shame. 

I don't think copying code. I mean, like little micro modules like some. Yeah, there's a balance, I guess. Like you don't want a million modules from like a million people like somebody like it would be cool to have like a standard library. I think Dino is doing this and like if it wins, it's going to be because of that. But like also like like it's an incredible capability to be able to like version code and distributed like in distributed systems accurately and reliably. And so I'm really always like astounded and floored that it's not used more in professional teams. Everyone just wants this monorepo that gets just deployed as this like distributed monolith that like breaks every time you deploy it for two seconds or for like 10 minutes as the versions resolved. 


## P13


Honestly, we update the main branch fairly rarely at this point. And we don't like I think if I look in the file, like it's the part policy to manage like the versioning is pretty lax. It doesn't really matter about the like minor version or anything. It's really just the major versions. So I think we really only update them. If for some reason, there's a problem, like if we're like, you know, doing something and we realize, okay, we can no longer have this version, or we need a specific version, then that's really the only time that we ever like actively update. the versioning. Honestly, I should probably have taken a look at this before this meeting and seen what the last few times we updated it. But yeah, I guess it's fairly rare. And then it's just whatever works. I don't think we're we're particularly diligent about making sure that everything is as best as possible. 

I just opened this online. I could even share but it seems like we only really update the dependency file. We only updated it three times last year. We haven't updated it all this year. So it is it is quite rare. I think the last change that we did in October 7 was because of an error. So it was like pi CDD lib. And again, since I'm not super an expert on on these like dependencies, like obviously if I had a lockfile, you know, this probably wouldn't have happened. But since I didn't have a lockfile, something changed, some dependency changed, and I now needed this explicit dependency. And it really took me a while to like figure out that this was wrong. It took me, I would say like at least a day if I remember correctly, of just like messing around, going online trying to figure out what was happening. And I finally settled that I needed this very explicit dependency. 

I think the reason that we don't care very much is because this is just it's like our project is essentially something that, you know, PhD students in robotics use like this is not critical. It doesn't really matter if it stops working for a day. And usually you only build the environment, you know, once every six months or something. So even if something breaks, like nobody's going to notice for a while. I think the only reason we noticed is because a new user came and they messaged in our, you know, our GitHub repo and was like, oh, like I'm having this trouble. And so I tried to replicate it and eventually got it working. But that really comes up, I would say, relatively rarely. 

I didn't create the gym. I joined the lab after the gym was created. I don't remember when like poetry and everything was like added to it. So I really wasn't in charge of that decision and I haven't really thought about it since. 

I do get like GitHub notifications. Like I recently got an email about like some there's like some SQL injection that can be done with Torch, with like the version of Torch that I'm using or something. I don't care. You know what I mean? Like this is again, this is like a developer thing that only PhD students are using on their local computer. So most of the time when there's, you know, some sort of vulnerability, I just, I kind of ignore it. Sometimes I'll fix it just to get the notification out of my face. But for the most part, it's like pretty minor. 

The only other organization that really assists me is, I don't know if you have heard of or have ever worked with the Farama Foundation. They're an NGO, I think based out of the US, that specifically tried to standardize reinforcement learning. And they like we use some code from them. And I guess a lot of the reinforcement learning community uses code from them. And they very occasionally will message us, since they know that we work with their APIs and their stuff, and be like, hey, like this dependency is out of date. We're actually trying to like push this latest thing, like, please update your, you know, whatever your code in this way to be compliant with our latest standards. And then we'll update our dependencies and our code to match whatever the Farama Foundation tells us to do. 

Frankly, we should. Like, that's, you know, like, you know, in retrospect, yeah, like 100%, that's probably important. And we somewhat advertise in our papers that it is reproducible. But, you know, without the lock file, I agree, it is not reproducible. I think maybe one of the reasons we don't care as much is a lot of the code that we use is not reproducible anyways. Like, I think PyTorch in certain instances is just, it's not reproducible. Like, it has no guarantees of reproducibility, even if you set all the seeds correctly. And, for example, I use like an optimization software that is, it will certainly return different results on every, like, computer architecture, essentially. Like, even a similar operating system, even a similar whatever, it will return different results. So, even though we do care about reproducibility, you know, kind of in theory, it's either very hard to do or impossible to do, even with a lock file, that we kind of don't worry too much about it. Even though we somewhat say in our papers, like, oh, like the, you know, you can reproduce the code here and, you know, you'll get almost the same results, but, you know, you won't actually get the same results. 

We really mostly use Python just because I think for reinforcement learning algorithms, it's really the standard. I do occasionally, and I know people in my lab do as well, use MATLAB for some more classical control stuff just because MATLAB has some, you know, like really pre-built tools that are very good. But very rarely will you build an entire project out of MATLAB. Like you'll take a piece and use it to like do some system identification or something and you'll have some scripts in MATLAB, but usually the kind of the overarching code is Python. So, poetry, not necessarily. Poetry, not necessarily. I think poetry might be very specific to this project because essentially this project was created by a postdoc who was more software engineering focused than the rest of us. And he created it and this was kind of his thing. And I'm pretty sure he's the one who chose to use poetry. And he's left now. But since this was his project, this is probably the, of all the projects in our lab, this is probably the better, you know, software architected project. 

Again, like we, the thing that I, that I think maybe differentiates us from other projects is like, realistically, I build my environment once. Mm-hmm. And then I won't rebuild it until there's an issue or for some reason I just had to like clone the repo again and, you know, rebuild it. So like, definitely I've gone, you know, six months without rebuilding the project. And so if it takes a while, like, I don't really care. 

And so just maybe a little bit about this project. So it's, it will, so this is not, is that embedded in the, in the, in the drones or in the flying robots or? No, essentially we've developed a way to kind of connect it to the drone, but in general, it's like a pure simulation environment. Okay. At least like, that's like the, the main way to use it is like, yeah, the, the real life drones, we kind of have a whole different thing going on for them. 

Honestly, it's a good point if we ever decide to focus more on reproducibility to, um, attempt to look into lockfiles, but it, yeah, I guess if we worry about reproducibility, we will have to look more into whether our optimizers and PyTorch are reproducible anyways, which I'm, I'm not a hundred percent sure, but that's a good point for me to think about, uh, in the future as well. 
